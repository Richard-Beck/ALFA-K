---
title: "Comparing fit quality"
author: "Richard J Beck"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir="~/projects/008_birthrateLandscape/ALFA-K/")
```

**Here, we aim to evaluate whether fitting hTERTa and hTERTb is best done using different fitness landscapes, or using a single combined landscape**

```{r,message=F,result=F,warning=F}
library(ggplot2)
source("utils/comparison_functions.R")
source("utils/ALFA-K.R")
```

Before diving into the analysis, it is useful to know that there are several "frequent clones" shared between hTERTa and hTERTb, and there is some (but not an overwhelming amount of) correlation between our fitness estimates on them.

```{r,echo=FALSE}
dir <- "figures/comparing_fit_quality/loo_xv/"
hTERTa <- readRDS(paste0(dir,"hTERTa.Rds"))
hTERTb <- readRDS(paste0(dir,"hTERTb.Rds"))
rx <- rownames(hTERTa)[rownames(hTERTa)%in%rownames(hTERTb)]
plot(hTERTa[rx,"f_est"],hTERTb[rx,"f_est"],xlab = "hTERTa fitness est.",ylab="hTERTb fitness est.")

print(paste("correlation coefficient between independent fitness estimates is:", 
      round(cor(hTERTa[rx,"f_est"],hTERTb[rx,"f_est"]),digits=2)))

```

**1. Cross validation**

The first thing we try is the leave-one-out cross validation procedure. This is done using the following script (which takes a while, so the intermediate results are saved):

```{r,eval=F}
source("figures/comparing_fit_quality/loo_xv.R")
```

```{r,echo=F}


xsep <- rbind(hTERTa,hTERTb)
xcomb <-  readRDS(paste0(dir,"hTERTcomb.Rds"))
#xcomb$loo_pred <- 0.2+xcomb$loo_pred-min(xcomb$loo_pred)
#xcomb$f_est <- 0.2+xcomb$f_est-min(xcomb$f_est)

xsep$id2 <- "separate"
xcomb$id2 <- "combined"

x <- rbind(xsep,xcomb)


p <- ggplot(x,aes(x=f_est,y=loo_pred,color=id2))+
  geom_point()+
  geom_abline()+
  scale_x_continuous("direct fitness estimate")+
  scale_y_continuous("Krig fitness estimate")+
  scale_color_discrete("landscape\ntype")
p

RMSE <- function(obs,pred){
  round(sqrt(mean((obs-pred)^2)),digits=4)
}

df_smm <- data.frame(value=c(RMSE(xsep$f_est,xsep$loo_pred),
                             RMSE(xcomb$f_est,xcomb$loo_pred),
                             R2(xsep$f_est,xsep$loo_pred),
                             R2(xcomb$f_est,xcomb$loo_pred),
                             cor(xsep$f_est,xsep$loo_pred)^2,
                             cor(xcomb$f_est,xcomb$loo_pred)^2),
                     id = rep(c("separate","combined"),3),
                     metric = c("RMSE","RMSE","R^2","R^2","r^2","r^2"))


p <- ggplot(df_smm,aes(x=id,y=value))+
  facet_wrap(~metric,scales="free",nrow=1,labeller = "label_parsed")+
  geom_col()+
  scale_y_continuous("metric value")+
  scale_x_discrete("landscape type")
p

```

```{r,echo=FALSE}

separate.df <- readRDS("example_data/hTERTa_fit.Rds")$fit$eff.df+
        readRDS("example_data/hTERTb_fit.Rds")$fit$eff.df
combined.df <- readRDS("example_data/hTERTcomb_fit.Rds")$fit$eff.df

print(paste("combined fit has",combined.df,"degrees of freedom, separate fit has",separate.df,"degrees of freedom."))

```

Interpretation of the above results is as follows:

1)  separate landscapes scores better on all metrics.
2)  Combined landscape scores *extremely* poorly on the $R^2$ metric.
3)  The reason that the combined landscape scores so poorly on the $R^2$ metric is largely due to a bias i.e. tendency to always overestimate fitness values by a constant factor. It is not clear to me why this would happen. Notably, the separate landscape also seems to feel this effect although it is not clear exactly why.
4)  The separately fit landscapes have about 400 degrees of freedom more than the combined fit. I am not entirely clear what *degrees of freedom* means in this context (there are varying definitions), but the separately fit models should be penalized somehow to account for this. I do not know how to do so.

**2. Ability to predict evolution**

```{r,echo=FALSE,message=FALSE,warning=FALSE,eval=FALSE,result=FALSE}
## this chunk deletes all the files generated by the ABM but not used - this is to keep the total size of the github repository as small as possible, it is not strictly necessary
cond_mapr <- c(hTERTa="hTERTa",hTERTb="hTERTb",hTERTcomb_1="hTERTa",hTERTcomb_2="hTERTb")

dir <- "figures/comparing_fit_quality/evo_pred/"
conds <- list.files(dir)
for(ci in conds){
  
di <- paste0(dir,ci,"/out/")

fi <- list.files(di)
fi <- paste0(di,fi)
for(fij in fi){
  flist <- list.files(fij)
  sel <- paste0(stringr::str_pad(seq(0,1000,100),width = 5,pad=0))
  sel <- c(paste0(sel,".csv"),"log.txt","summary.txt")
  to_delete <- paste0(fij,"/",flist[!flist%in%sel])
  file.remove(to_delete)
}
}

```

Here we fit ALFA-K to the hTERTa and hTERTb datasets (either separately or combined), whilst excluding the final timepoints from the fit. If we denote $X_{n-1,a}$ and $X_{n,a}$ the karyotype distributions from the final two timepoints of hTERTa, then $W_{X,a}$ is the Wasserstein distance between $X_{n-1,a}$ and $X_{n,a}$. We start the ABM simulations (N=25 repeats) with an input karyotype distribution $X_{n-1,a}$ and simulate until $W_{X,a}=W_{Y,a}$, where $W_{Y,a}$ is the Wasserstein distance between $X_{n-1,a}$ and the simulated karyotype distribution $Y_{n,a}$. In other words, we simulate the ABMs until the population travels a distance from the penultimate sampled timepoint, which is equal to the distance between the final two timepoints of the experimental data. We then do the same for hTERTb.   

The code for running these ABM sims is here:
```{r,eval=F}
source("figures/comparing_fit_quality/evo_pred.R")
```

To evaluate how good our simulated predictions were, we compute the Wasserstein distances between $X_{n,a}$ and $Y_{n,a}$. Here we find the Wasserstein distances to be very similar whether populations evolve on separately trained or combined landscapes (perhaps even the combined landscape has a slight edge). Given the additional degrees of freedom for the separate fits, we would have to conclude that the combined landscape has done better on this test. 

```{r,message=F,warning=F,echo=F}

eval_wass <- function(ci){
  di <- paste0(dir,ci,"/out/")
  
  y <- readRDS(paste0("data/cellLines/02_optim_data/",cond_mapr[ci],".Rds"))
  ty <- as.numeric(tail(colnames(y$x),2))
  y1 <- make_wass_object(y,t=ty[1])
  y2 <- make_wass_object(y,t=ty[2])
  dtarg <- get_dwass(y1,y2)
  
  fi <- list.files(di)
  fi <- paste0(di,fi)
  tt <- seq(0,1000,100)
  
  x <- lapply(fi, function(fij){
    proc_sim(fij,times=tt)
  })
  
  d <- sapply(tt, function(t2){
    mean(sapply(x,function(xij){
      x1 <- make_wass_object(xij,t=0)
      x2 <- make_wass_object(xij,t=t2)
      get_dwass(x1,x2)
    }))
  })
  d <- d[-1]
  tt <- tt[-1]
  tt <- tt[which.min(abs(d-dtarg))]
  
  x <- lapply(x,function(xi){
    make_wass_object(xi,t=tt)
  })
  
  dwass <- unlist(lapply(1:length(x), function(i){
    unlist(lapply(i:length(x), function(j){
      if(i==j) return(NULL)
      get_dwass(x[[i]],x[[j]])
    }))
  }))
  
  dwass_tst <- sapply(x,function(xi) get_dwass(y2,xi))
  
  df <- rbind(data.frame(dwass=dwass,id="train"),
              data.frame(dwass=dwass_tst,id="test"))
  df$predicted_population <- cond_mapr[ci]
  df$landscape <- cond_mapr[ci]
  if(ci%in%c("hTERTcomb_1", "hTERTcomb_2")) {
    df$landscape <- "combined"
  }
  return(df)
}

cond_mapr <- c(hTERTa="hTERTa",hTERTb="hTERTb",hTERTcomb_1="hTERTa",hTERTcomb_2="hTERTb")

dir <- "figures/comparing_fit_quality/evo_pred/"
conds <- list.files(dir)
df <- do.call(rbind,lapply(conds,eval_wass))
df2 <- df[df$id=="test",]

p <- ggplot(df2,aes(x=dwass,color=landscape))+
  facet_wrap(~predicted_population)+
  stat_ecdf(geom="step")+
  scale_x_continuous("Wasserstein distance")+
  scale_y_continuous("cumulative distribution")
p

```

Although in the above test the combined landscape performed better, if instead the separately fit landscapes had performed better we would have been left with the same problem as with the leave-one-out comparison - namely how to penalize for the additional degrees of freedom. This problem could be solved by using the ABM simulations to generate a *reference distribution*. For this we calculate the Wasserstein distances $W_{X,X}$ between all pairs of simulation output. We can then ask what is the *likelihood* that we obtain a distance equal to $W_{X,Y}$ (which is the distance between a simulation and our data). Having a likelihood would allow using model selection techniques e.g. AIC to quantify the effect of additional degrees of freedom (although I didn't code this yet). Notably with the Wasserstein metric in particular for hTERTb it appears essentially impossible that we would get $W_{X,Y}$ (the ABM outputs are all very similar and quite different to the data), which raises the question of whether our simulation output was predictive at all. 
```{r,echo=FALSE}

df$landscape <- paste("landscape:",df$landscape)
df$predicted_population <- paste("predicted:",df$predicted_population)
p <- ggplot(df,aes(x=dwass,color=id))+
  facet_wrap(landscape~predicted_population)+
  stat_ecdf(geom="step")+
  scale_x_continuous("Wasserstein distance")+
  scale_y_continuous("cumulative distribution")+
  scale_color_discrete(labels=c(expression(W[X~','~Y]),
                                expression(W[X~','~X])))
p

```

Alternatively to using the Wasserstein metric we can use the evolution angle metric which compares the angle through karyotype space taken by the ABM simulations to the angle taken by the experimental population (lower = better). Here again we see similar predictive ability comparing combined v.s. separate landscapes.  

```{r,message=F,warning=F,echo=F}

eval_angle <- function(ci){
  di <- paste0(dir,ci,"/out/")
fi <- list.files(di)
fi <- paste0(di,fi)
y <- readRDS(paste0("data/cellLines/02_optim_data/",cond_mapr[ci],".Rds"))
ty <- as.numeric(tail(colnames(y$x),2))

y1 <- get_mean(y,ty[1])
y2 <- get_mean(y,ty[2])

vy <- y2-y1
dtarg <- sqrt(mean(vy^2))

tt <- seq(0,1000,100)
x <- lapply(fi,function(fij){
  proc_sim(fij,times=tt)
})

d <- 0
use_wasserstein <- TRUE
if(use_wasserstein){
d <- sapply(tt, function(t2){
  mean(sapply(x,function(xij){
    x1 <- make_wass_object(xij,t=0)
    x2 <- make_wass_object(xij,t=t2)
    get_dwass(x1,x2)
  }))
})
}else{
d <- sapply(tt, function(t2){
  mean(sapply(x,function(xij){
    x1 <- get_mean(xij,0)
    x2 <- get_mean(xij,t2)
    vx <- x2-x1
    sqrt(mean(vx^2))
  }))
})
}
tt <- tt[-1]
d <- d[-1]

tt <- tt[which.min(abs(d-dtarg))]


amet <- unlist(lapply(1:length(x), function(i){
  unlist(lapply(i:length(x), function(j){
    if(i==j) return(NULL)
    x0 <- get_mean(x[[i]],0)
    x1 <- get_mean(x[[i]],tt)-x0
    x2 <- get_mean(x[[j]],tt)-x0
    getangle(x1,x2)
  }))
}))

amet_tst <- sapply(x,function(xi){
    x0 <- get_mean(xi,0)
    x1 <- get_mean(xi,tt)-x0
    getangle(x1,vy)
})

df <- rbind(data.frame(amet=amet,id="train"),
            data.frame(amet=amet_tst,id="test"))
  df$predicted_population <- cond_mapr[ci]
  df$landscape <- cond_mapr[ci]
  if(ci%in%c("hTERTcomb_1", "hTERTcomb_2")) {
    df$landscape <- "combined"
  }
  return(df)
}

cond_mapr <- c(hTERTa="hTERTa",hTERTb="hTERTb",hTERTcomb_1="hTERTa",hTERTcomb_2="hTERTb")

dir <- "figures/comparing_fit_quality/evo_pred/"
conds <- list.files(dir)
df <- do.call(rbind,lapply(conds,eval_angle))
df2 <- df[df$id=="test",]

p <- ggplot(df2,aes(x=amet,color=landscape))+
  facet_wrap(~predicted_population)+
  stat_ecdf(geom="step")+
  scale_x_continuous("evolution angle")+
  scale_y_continuous("cumulative distribution")
p

```
Again the angles between different simulation replicates $A_{X,X}$ are way smaller than the angle between data and sims $A_{X,Y}$, implying that our sims are all quite similar and we are unlikely to generate a simulated population that looks like the experimental data. However, since the angles $A_{X,Y}$ are (almost all) below 90 degrees, at least we know that our landscape predicts better than random chance. 

```{r,echo=FALSE}

df$landscape <- paste("landscape:",df$landscape)
df$predicted_population <- paste("predicted:",df$predicted_population)
p <- ggplot(df,aes(x=amet,color=id))+
  facet_wrap(landscape~predicted_population)+
  stat_ecdf(geom="step")+
  scale_x_continuous("evolution angle")+
  scale_y_continuous("cumulative distribution")+
  scale_color_discrete(labels=c(expression(A[X~','~Y]),
                                expression(A[X~','~X])))
p

```

**Conclusions**

1. I still don't know whether it is better to fit hTERTa and hTERTb together or separate.
2. With a little extra work we can do model selection using either the Wasserstein or angle metrics and come to a conclusion.
3. It is weird that the predictions from the leave one out procedure are biased and it seems wise to investigate why this is the case. 
